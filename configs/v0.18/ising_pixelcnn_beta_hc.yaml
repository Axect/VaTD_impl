# v0.18 PixelCNN with Beta-Conditioned HC
#
# Extends HC fusion with a small MLP that maps β (inverse temperature) to
# per-sample mixing weights. Near the critical temperature Tc≈2.269, the
# system undergoes a phase transition — this allows the fusion layer to
# adapt its strategy dynamically based on temperature.
#
# A100-optimized: Large batch (2048) with fewer accumulation steps (4).
# Same total samples/epoch as v0.16 (65,536), but 4x better RLOO baseline
# per step and reduced CUDA kernel launch overhead.

project: Ising_VaTD_v0.18
device: cuda:0
net: model.DiscretePixelCNN
optimizer: torch.optim.AdamW
scheduler: hyperbolic_lr.ExpHyperbolicLR
epochs: 250
batch_size: 2048
seeds: [42]

net_config:
  # Lattice configuration
  size: 16
  fix_first: 1

  # Temperature sampling configuration
  # batch_size × num_beta × accumulation_steps = 2048 × 8 × 4 = 65,536 samples/epoch
  batch_size: 2048
  num_beta: 8
  beta_min: 0.2
  beta_max: 1.0

  # Training Mode: Sequential
  # 4 accum steps with batch=2048 gives same total samples as 16 steps with batch=512,
  # but with 4x lower RLOO baseline variance per optimizer step.
  training_mode: sequential
  accumulation_steps: 4

  # MCMC Guided Training: DISABLED (pure REINFORCE)
  mcmc_enabled: false

  # A100 CUDA backend: cuDNN autotuner finds optimal conv algorithms
  cudnn_benchmark: true

  # 2-Phase Curriculum Learning
  curriculum_enabled: true
  phase1_epochs: 50
  phase1_beta_max: 0.35
  phase2_epochs: 100

  # Model architecture (4-layer Dilated ResNet)
  kernel_size: 7
  hidden_channels: 96
  hidden_conv_layers: 4
  hidden_kernel_size: 3
  hidden_width: 128
  hidden_fc_layers: 2

  # Dilation pattern for exponential receptive field
  dilation_pattern: [1, 2, 4, 8]

  # Multi-scale skip connections
  skip_connection_indices: [1, 3]

  # Beta-Conditioned HC Fusion
  use_mhc_fusion: true
  mhc_config:
    manifold_constraint: false
    use_dynamic_h: true
    aggregation: sum
    beta_conditioned: true
    beta_mlp_hidden: 32

  # Temperature-dependent Output Scaling (Parameter-free)
  logit_temp_scale: true
  temp_scale_min: 0.1
  temp_scale_max: 2.0

optimizer_config:
  lr: 1.e-3
  weight_decay: 1.e-4

scheduler_config:
  upper_bound: 300
  max_iter: 250
  infimum_lr: 1.e-6

early_stopping_config:
  enabled: false
