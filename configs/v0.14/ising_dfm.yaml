# v0.14: Discrete Flow Matching for Ising Model
#
# Key changes from v0.13 (PixelCNN):
# - Replaced autoregressive PixelCNN with Discrete Flow Matching
#   * Parallel sampling via Euler integration (50 steps vs 256 sequential)
#   * Dirichlet probability path for noising
#   * Energy-guided training (MH-improved targets + velocity matching)
#
# Advantages:
# - ~3-5x faster sampling (parallel vs autoregressive)
# - Direct energy guidance ensures samples converge to Boltzmann distribution
# - More flexible architecture (no masking constraints)
#
# Training approach:
# - Energy-guided mode: Denoise toward MH-improved samples + velocity matching
# - Mean-field Boltzmann velocity guides sampling toward low-energy states
# - Same curriculum learning as v0.13

project: Ising_DFM_v0.14
device: cuda:0
net: model_dfm.DiscreteFlowMatcher
optimizer: splus.SPlus
scheduler: hyperbolic_lr.ExpHyperbolicLR
epochs: 300
batch_size: 256
seeds: [42]

net_config:
  # Lattice configuration
  size: 16
  fix_first: 1

  # Temperature sampling configuration
  batch_size: 64            # Samples per temperature (reduced for GPU memory)
  num_beta: 4               # Number of temperature samples per step
  beta_min: 0.2             # 1/T_max (T_max = 5) - narrower than validation for extrapolation test
  beta_max: 1.0             # 1/T_min (T_min = 1) - narrower than validation for extrapolation test

  # Flow Matching Parameters
  # For Dirichlet path, convergence = 1 - 2/(2+t_max)
  # t_max=50 gives ~96% convergence, t_max=100 gives ~98%
  num_flow_steps: 100       # Euler integration steps for sampling
  t_max: 50.0               # Maximum integration time (was 5.0, too small!)
  t_min: 0.01               # Avoid t=0 singularity
  time_dim: 64              # Time embedding dimension

  # Training Mode
  training_mode: energy_guided  # "energy_guided" (MH-improved + velocity matching)
  energy_weight: 1.0            # Weight for velocity matching loss
  mh_steps: 100                 # MH sweeps for equilibration (checkerboard, ~200 half-lattice updates)
  lambda_reinforce: 0.1         # (unused in energy_guided mode)
  accumulation_steps: 8         # Gradient accumulation steps per epoch

  # 2-Phase Curriculum Learning (same as v0.13)
  curriculum_enabled: true
  phase1_epochs: 75         # High temp learning
  phase1_beta_max: 0.35     # T_min = 2.86 (above Tc)
  phase2_epochs: 100        # Gradual expansion to full range

  # Model architecture (ResNet without masking)
  hidden_channels: 64       # ResNet block width
  hidden_conv_layers: 5     # Number of residual blocks
  hidden_kernel_size: 3     # Spatial kernel size
  hidden_width: 128         # FC layer width
  hidden_fc_layers: 2       # Number of FC layers

  # Temperature-dependent Output Scaling (optional)
  logit_temp_scale: false   # Disabled by default for DFM
  temp_ref: 2.27            # Reference temperature (Tc)
  temp_scale_power: 0.5
  temp_scale_min: 0.1
  temp_scale_max: 10.0

optimizer_config:
  lr: 2.e-1
  eps: 1.e-10

scheduler_config:
  upper_bound: 350          # Slightly larger than epochs
  max_iter: 300             # Same as epochs
  infimum_lr: 1.e-6         # Final learning rate

early_stopping_config:
  enabled: false
