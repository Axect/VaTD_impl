# v0.14.1: Discrete Flow Matching + OneCycleLR
#
# Key changes from v0.14:
# - OneCycleLR scheduler for better convergence
#   * Warmup phase: stabilizes noisy gradients
#   * High LR phase: escapes local minima
#   * Annealing phase: fine-tunes to convergence
#
# DFM features:
# - Parallel sampling via Euler integration (50 steps)
# - Dirichlet probability path for noising
# - Pure REINFORCE training (self-training without external data)
# - Free energy minimization: F = log q + β·E

project: Ising_DFM_v0.14
device: cuda:0
net: model_dfm.DiscreteFlowMatcher
optimizer: torch.optim.AdamW
scheduler: torch.optim.lr_scheduler.OneCycleLR
epochs: 300
batch_size: 256
seeds: [42]

net_config:
  # Lattice configuration
  size: 16
  fix_first: 1

  # Temperature sampling configuration
  batch_size: 64            # Samples per temperature
  num_beta: 4               # Number of temperature samples per step
  beta_min: 0.2             # 1/T_max (T_max = 5)
  beta_max: 1.0             # 1/T_min (T_min = 1)

  # Flow Matching Parameters
  num_flow_steps: 50        # Euler integration steps for sampling
  t_max: 5.0                # Maximum integration time
  t_min: 0.01               # Avoid t=0 singularity
  time_dim: 64              # Time embedding dimension

  # Training Mode
  training_mode: reinforce  # "reinforce" (pure REINFORCE for self-training)
  lambda_reinforce: 0.1     # Weight for REINFORCE term (ignored in reinforce mode)
  accumulation_steps: 8     # Gradient accumulation steps per epoch

  # 2-Phase Curriculum Learning
  curriculum_enabled: true
  phase1_epochs: 75         # High temp learning
  phase1_beta_max: 0.35     # T_min = 2.86 (above Tc)
  phase2_epochs: 100        # Gradual expansion to full range

  # Model architecture (ResNet without masking)
  hidden_channels: 64       # ResNet block width
  hidden_conv_layers: 5     # Number of residual blocks
  hidden_kernel_size: 3     # Spatial kernel size
  hidden_width: 128         # FC layer width
  hidden_fc_layers: 2       # Number of FC layers

  # Temperature-dependent Output Scaling (optional)
  logit_temp_scale: false
  temp_ref: 2.27
  temp_scale_power: 0.5
  temp_scale_min: 0.1
  temp_scale_max: 10.0

optimizer_config:
  lr: 1.e-3                 # Base LR (used as max_lr reference)
  weight_decay: 1.e-4

scheduler_config:
  max_lr: 3.e-3             # Peak LR (3x base)
  total_steps: 2400         # epochs * accumulation_steps = 300 * 8
  pct_start: 0.25           # 25% warmup
  anneal_strategy: cos      # Cosine annealing
  div_factor: 10            # initial_lr = 3e-4
  final_div_factor: 100     # final_lr = 3e-6

early_stopping_config:
  enabled: false
