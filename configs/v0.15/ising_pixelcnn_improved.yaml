# v0.15: Improved PixelCNN for Ising Model
#
# Key Architectural Improvements:
# - Dilated Convolutions: Receptive field expands exponentially (1, 2, 4, 8).
# - MCMC Replay Buffer: Persistent "Perfect Samples" updated via Swendsen-Wang.
# - Symmetry Augmentation: Random rotations and flips during MCMC guidance.
#
# Training Strategy:
# - mcmc_freq: 1 (Every epoch)
# - mcmc_sw_sweeps: 20 (Better buffer equilibration)
# - hidden_conv_layers: 8 (Deep dilated network)

project: Ising_VaTD_v0.15
device: cuda:0
net: model.DiscretePixelCNN
optimizer: torch.optim.AdamW
scheduler: hyperbolic_lr.ExpHyperbolicLR
epochs: 500
batch_size: 512
seeds: [42]

net_config:
  # Lattice configuration
  size: 16
  fix_first: 1

  # Temperature sampling configuration
  batch_size: 256       
  num_beta: 8           
  beta_min: 0.2         
  beta_max: 1.0         

  # Training Mode: Sequential
  training_mode: sequential
  accumulation_steps: 64      

  # MCMC Guided Training (Improved with Replay Buffer & Augmentation)
  mcmc_enabled: true
  mcmc_freq: 1        # Guided every epoch
  mcmc_weight: 1.0    
  mcmc_sw_sweeps: 20  # Stronger equilibration for buffer
  mcmc_mh_steps: 0    

  # 2-Phase Curriculum Learning
  curriculum_enabled: true
  phase1_epochs: 100          
  phase1_beta_max: 0.35       
  phase2_epochs: 150          

  # Model architecture (Dilated ResNet)
  kernel_size: 7              
  hidden_channels: 96         
  hidden_conv_layers: 8       # Increased depth for dilation steps
  hidden_kernel_size: 3       # Small kernel + Dilation = Large RF
  hidden_width: 128           
  hidden_fc_layers: 2

  # Temperature-dependent Output Scaling
  logit_temp_scale: true
  temp_ref: 2.27              
  temp_scale_power: 1.0       
  temp_scale_min: 0.1         
  temp_scale_max: 10.0        

optimizer_config:
  lr: 1.e-3
  weight_decay: 1.e-4  # Added slight decay for stability

scheduler_config:
  upper_bound: 600
  max_iter: 500
  infimum_lr: 1.e-6

early_stopping_config:
  enabled: false