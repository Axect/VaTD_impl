# v0.11: Gradient Accumulation Training (Fixed)
#
# Key fixes from original v0.11:
# - Multiple optimizer steps per epoch (like refs/vatd)
# - RLOO baseline (better variance reduction than simple mean)
# - AMP disabled (numerical stability for REINFORCE)
# - Moderate kernel size (balance between local/global correlation)
# - Extended Phase 1 for better high-temp learning
#
# refs/vatd does 19 optimizer steps per epoch (115 betas / 6)
# This version: num_batches_per_epoch optimizer steps

project: Ising_VaTD_v0.11
device: cuda:0
net: model.DiscretePixelCNN
optimizer: torch.optim.AdamW
scheduler: hyperbolic_lr.ExpHyperbolicLR
epochs: 500
batch_size: 512
seeds: [42]

net_config:
  # Lattice configuration
  size: 16
  fix_first: 1

  # Temperature sampling configuration
  batch_size: 128       # Samples per temperature (per micro-step)
  num_beta: 8           # Number of temperature samples per micro-step
  beta_min: 0.2         # 1/T_max (T_max = 5)
  beta_max: 1.0         # 1/T_min (T_min = 1)

  # v0.11 Training Mode: Gradient Accumulation (Fixed)
  training_mode: accumulated
  accumulation_steps: 4       # Micro-steps per optimizer update
  num_batches_per_epoch: 16   # Optimizer steps per epoch (like refs/vatd's 19)
  use_amp: false              # Disabled for numerical stability

  # 3-Phase Curriculum Learning (extended Phase 1)
  curriculum_enabled: true
  phase1_epochs: 100          # Extended: High temp learning (was 50)
  phase1_beta_max: 0.35       # T_min = 2.86 (above Tc)
  phase2_epochs: 150          # Extended: Gradual expansion (was 100)
  tc_focus_ratio: 0.5         # Phase 3: 50% Tc region focus
  tc_beta_min: 0.38           # T = 2.63
  tc_beta_max: 0.52           # T = 1.92

  # Model architecture (balanced kernel for all temperatures)
  kernel_size: 7              # Moderate receptive field (was 13)
  hidden_channels: 96         # Increased capacity (was 64)
  hidden_conv_layers: 6       # refs/vatd default
  hidden_kernel_size: 5       # Smaller for high-temp local correlation (was 13)
  hidden_width: 128           # Increased capacity (was 64)
  hidden_fc_layers: 2         # refs/vatd default

optimizer_config:
  lr: 1.e-3
  weight_decay: 0.0           # No weight decay (refs/vatd style)

scheduler_config:
  upper_bound: 600
  max_iter: 500
  infimum_lr: 1.e-6

early_stopping_config:
  enabled: false
