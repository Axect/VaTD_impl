# v0.13: Sequential Training (refs/vatd style) with MCMC Guidance
#
# Key improvements from v0.12:
# - Sequential backward per temperature (no crosstalk)
#   * Parallel sampling (fast - 255 autoregressive steps done once)
#   * Sequential log_prob + backward per T (correct gradients)
#   * Matches refs/vatd behavior exactly
#
# v0.13.1: Increased training intensity
# - accumulation_steps: 16 → 64 (4x more optimizer steps)
# - batch_size per T: 128 → 256 (2x more samples, more stable RLOO)
# - Total optimizer steps: 64 × 500 = 32,000 (vs 8,000 before)
#
# Previous features retained:
# - Temperature-dependent Output Scaling (logit_temp_scale)
# - Zero bias initialization for spin symmetry
# - 2-Phase Curriculum Learning
#
# New Feature: MCMC Guided Training (Kalman Filter Style)
# - Periodically corrects samples using Swendsen-Wang MCMC
# - Applies MLE loss (Teacher Forcing) to guide model towards true distribution

project: Ising_VaTD_v0.13_MCMC
device: cuda:0
net: model.DiscretePixelCNN
optimizer: torch.optim.AdamW
scheduler: hyperbolic_lr.ExpHyperbolicLR
epochs: 500
batch_size: 512
seeds: [42]

net_config:
  # Lattice configuration
  size: 16
  fix_first: 1

  # Temperature sampling configuration
  batch_size: 256       # Samples per temperature (increased from 128)
  num_beta: 8           # Number of temperature samples per step
  beta_min: 0.2         # 1/T_max (T_max = 5)
  beta_max: 1.0         # 1/T_min (T_min = 1)

  # Training Mode: Sequential (v0.13)
  # Parallel sampling + Sequential backward per T
  training_mode: sequential
  accumulation_steps: 64      # Optimizer steps per epoch (increased from 16)

  # MCMC Guided Training (Kalman Filter style)
  mcmc_enabled: true
  mcmc_freq: 5        # Run correction every 5 epochs
  mcmc_steps: 10      # Number of Swendsen-Wang sweeps
  mcmc_weight: 1.0    # Weight for MLE loss

  # 2-Phase Curriculum Learning
  curriculum_enabled: true
  phase1_epochs: 100          # High temp learning
  phase1_beta_max: 0.35       # T_min = 2.86 (above Tc)
  phase2_epochs: 150          # Gradual expansion to full range

  # Model architecture
  kernel_size: 7              # Moderate receptive field
  hidden_channels: 96         # Increased capacity
  hidden_conv_layers: 6
  hidden_kernel_size: 5       # Smaller for high-temp local correlation
  hidden_width: 128           # Increased capacity
  hidden_fc_layers: 2

  # Temperature-dependent Output Scaling
  logit_temp_scale: true
  temp_ref: 2.27              # Reference temperature (Tc)
  temp_scale_power: 1.0       # Strong high-temp suppression
  temp_scale_min: 0.1         # Clamp minimum
  temp_scale_max: 10.0        # Clamp maximum

optimizer_config:
  lr: 1.e-3
  weight_decay: 0.0

scheduler_config:
  upper_bound: 600
  max_iter: 500
  infimum_lr: 1.e-6

early_stopping_config:
  enabled: false
